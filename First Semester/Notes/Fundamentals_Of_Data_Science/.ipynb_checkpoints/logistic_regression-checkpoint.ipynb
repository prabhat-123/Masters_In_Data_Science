{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e64aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8740ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d828c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abd7ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1eed992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990842, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06832974, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286377, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04687948,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452837, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00421986,  0.00306441]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c679b572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a232998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(d):\n",
    "    mean = d.mean()\n",
    "    std = d.std()\n",
    "    return (d - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4184ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    \n",
    "    ## The examples are read at random, in no particular order\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = np.array(indices[i: min(i + batch_size, num_examples)])\n",
    "        \n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ccfda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    data = load_diabetes()\n",
    "    X = data['data']\n",
    "    y = data['target']\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d53b9e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\" \n",
    "    This is the scratch implementation of Logistic Regression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.param = {}\n",
    "        self.m, self.n = X.shape\n",
    "        self.param['W'] = np.random.randn(self.n, 1) * 0.001\n",
    "        self.param['b'] = np.zeros(1)\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.result = pd.DataFrame()\n",
    "        \n",
    "    \n",
    "    def train(self, alpha = 0.001, epochs = 10):\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch: \", epoch, end = \"\")\n",
    "            z = np.dot(self.X, self.param[\"W\"]) + self.param['b']\n",
    "            \n",
    "            self.y_pred = self.sigmoid(z)\n",
    "            # print(y_pred)\n",
    "            self.result[0] = self.y\n",
    "            \n",
    "            ## Update the parameters\n",
    "            self.param['W'] = self.param['W'] - alpha * 1/self.m * np.dot(self.X.transpose(), \n",
    "                                                                         (self.y_pred - np.reshape(self.y,\n",
    "                                                                                                 (self.m, 1))))\n",
    "                                                                          \n",
    "            self.param['b'] = self.param['b'] - alpha * 1/self.m * np.sum(self.y_pred - np.reshape(self.y, \n",
    "                                                                                                  (self.m, 1)))\n",
    "                                                                \n",
    "            self.y_pred = self.sigmoid(np.dot(self.X, self.param['W']) + self.param['b'])\n",
    "                                                                          \n",
    "            loss = self.loss(self.y, self.y_pred)\n",
    "            \n",
    "            self.result[1] = self.y_pred\n",
    "            \n",
    "            print(\", loss = \", loss)\n",
    "                                                            \n",
    "        print(\", Final Loss = \", loss)\n",
    "        print(\"  W: {}, b = {}\".format(self.param['W'], self.param['b']))\n",
    "                                                                          \n",
    "                                                                          \n",
    "    def loss(self, y, y_pred):\n",
    "        # print(np.log(1 - y_pred))\n",
    "        y_zero_loss = y.T.dot(np.log(y_pred))\n",
    "        y_one_loss = (1 - y).T.dot(np.log(1 - y_pred))\n",
    "        \n",
    "        return -np.sum(y_zero_loss + y_one_loss)/ len(y)\n",
    "                                                                          \n",
    "                                                                          \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1 + np.exp(-z))\n",
    "                                                                          \n",
    "                                                                          \n",
    "    def predict(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.param['W']) + self.param['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "047dcea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5fe9cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0, loss =  0.6937708131928414\n",
      "Epoch:  1, loss =  0.6935406903323539\n",
      "Epoch:  2, loss =  0.69331056747292\n",
      "Epoch:  3, loss =  0.6930804446147206\n",
      "Epoch:  4, loss =  0.6928503217576736\n",
      "Epoch:  5, loss =  0.6926201989017459\n",
      "Epoch:  6, loss =  0.692390076046954\n",
      "Epoch:  7, loss =  0.6921599531933474\n",
      "Epoch:  8, loss =  0.6919298303408272\n",
      "Epoch:  9, loss =  0.6916997074895088\n",
      "Epoch:  10, loss =  0.691469584639359\n",
      "Epoch:  11, loss =  0.6912394617903452\n",
      "Epoch:  12, loss =  0.6910093389424342\n",
      "Epoch:  13, loss =  0.6907792160957085\n",
      "Epoch:  14, loss =  0.6905490932500857\n",
      "Epoch:  15, loss =  0.690318970405681\n",
      "Epoch:  16, loss =  0.6900888475624122\n",
      "Epoch:  17, loss =  0.6898587247202298\n",
      "Epoch:  18, loss =  0.6896286018792985\n",
      "Epoch:  19, loss =  0.6893984790394208\n",
      "Epoch:  20, loss =  0.6891683562007611\n",
      "Epoch:  21, loss =  0.6889382333632538\n",
      "Epoch:  22, loss =  0.6887081105269153\n",
      "Epoch:  23, loss =  0.6884779876915973\n",
      "Epoch:  24, loss =  0.6882478648575304\n",
      "Epoch:  25, loss =  0.6880177420246323\n",
      "Epoch:  26, loss =  0.6877876191928536\n",
      "Epoch:  27, loss =  0.6875574963622436\n",
      "Epoch:  28, loss =  0.68732737353272\n",
      "Epoch:  29, loss =  0.6870972507043488\n",
      "Epoch:  30, loss =  0.6868671278771793\n",
      "Epoch:  31, loss =  0.6866370050511785\n",
      "Epoch:  32, loss =  0.6864068822262642\n",
      "Epoch:  33, loss =  0.6861767594025187\n",
      "Epoch:  34, loss =  0.6859466365799418\n",
      "Epoch:  35, loss =  0.685716513758468\n",
      "Epoch:  36, loss =  0.6854863909381957\n",
      "Epoch:  37, loss =  0.6852562681191088\n",
      "Epoch:  38, loss =  0.6850261453011083\n",
      "Epoch:  39, loss =  0.6847960224841778\n",
      "Epoch:  40, loss =  0.6845658996685478\n",
      "Epoch:  41, loss =  0.6843357768539876\n",
      "Epoch:  42, loss =  0.6841056540406127\n",
      "Epoch:  43, loss =  0.6838755312283573\n",
      "Epoch:  44, loss =  0.6836454084172706\n",
      "Epoch:  45, loss =  0.6834152856073691\n",
      "Epoch:  46, loss =  0.6831851627985212\n",
      "Epoch:  47, loss =  0.6829550399909243\n",
      "Epoch:  48, loss =  0.6827249171844139\n",
      "Epoch:  49, loss =  0.6824947943790228\n",
      "Epoch:  50, loss =  0.6822646715748993\n",
      "Epoch:  51, loss =  0.6820345487717964\n",
      "Epoch:  52, loss =  0.6818044259699281\n",
      "Epoch:  53, loss =  0.6815743031691627\n",
      "Epoch:  54, loss =  0.6813441803695826\n",
      "Epoch:  55, loss =  0.6811140575711053\n",
      "Epoch:  56, loss =  0.6808839347738627\n",
      "Epoch:  57, loss =  0.6806538119776571\n",
      "Epoch:  58, loss =  0.6804236891826861\n",
      "Epoch:  59, loss =  0.6801935663888511\n",
      "Epoch:  60, loss =  0.6799634435961682\n",
      "Epoch:  61, loss =  0.6797333208045884\n",
      "Epoch:  62, loss =  0.6795031980141937\n",
      "Epoch:  63, loss =  0.6792730752249349\n",
      "Epoch:  64, loss =  0.6790429524368283\n",
      "Epoch:  65, loss =  0.678812829649907\n",
      "Epoch:  66, loss =  0.678582706864105\n",
      "Epoch:  67, loss =  0.6783525840794554\n",
      "Epoch:  68, loss =  0.6781224612959745\n",
      "Epoch:  69, loss =  0.6778923385135965\n",
      "Epoch:  70, loss =  0.6776622157324368\n",
      "Epoch:  71, loss =  0.6774320929523305\n",
      "Epoch:  72, loss =  0.6772019701734424\n",
      "Epoch:  73, loss =  0.6769718473956736\n",
      "Epoch:  74, loss =  0.6767417246190901\n",
      "Epoch:  75, loss =  0.676511601843626\n",
      "Epoch:  76, loss =  0.6762814790693635\n",
      "Epoch:  77, loss =  0.6760513562962039\n",
      "Epoch:  78, loss =  0.6758212335241638\n",
      "Epoch:  79, loss =  0.6755911107533088\n",
      "Epoch:  80, loss =  0.6753609879836391\n",
      "Epoch:  81, loss =  0.6751308652150394\n",
      "Epoch:  82, loss =  0.6749007424476579\n",
      "Epoch:  83, loss =  0.6746706196813956\n",
      "Epoch:  84, loss =  0.6744404969163351\n",
      "Epoch:  85, loss =  0.6742103741523281\n",
      "Epoch:  86, loss =  0.6739802513895558\n",
      "Epoch:  87, loss =  0.6737501286278699\n",
      "Epoch:  88, loss =  0.6735200058673692\n",
      "Epoch:  89, loss =  0.6732898831080207\n",
      "Epoch:  90, loss =  0.6730597603498246\n",
      "Epoch:  91, loss =  0.6728296375927315\n",
      "Epoch:  92, loss =  0.67259951483684\n",
      "Epoch:  93, loss =  0.6723693920820679\n",
      "Epoch:  94, loss =  0.672139269328448\n",
      "Epoch:  95, loss =  0.6719091465760464\n",
      "Epoch:  96, loss =  0.6716790238247312\n",
      "Epoch:  97, loss =  0.6714489010745848\n",
      "Epoch:  98, loss =  0.6712187783255412\n",
      "Epoch:  99, loss =  0.6709886555776994\n",
      "Epoch:  100, loss =  0.670758532830944\n",
      "Epoch:  101, loss =  0.6705284100854068\n",
      "Epoch:  102, loss =  0.6702982873409888\n",
      "Epoch:  103, loss =  0.6700681645977233\n",
      "Epoch:  104, loss =  0.66983804185561\n",
      "Epoch:  105, loss =  0.6696079191146491\n",
      "Epoch:  106, loss =  0.669377796374824\n",
      "Epoch:  107, loss =  0.6691476736362005\n",
      "Epoch:  108, loss =  0.6689175508986471\n",
      "Epoch:  109, loss =  0.6686874281622789\n",
      "Epoch:  110, loss =  0.6684573054270959\n",
      "Epoch:  111, loss =  0.6682271826930323\n",
      "Epoch:  112, loss =  0.6679970599600551\n",
      "Epoch:  113, loss =  0.667766937228329\n",
      "Epoch:  114, loss =  0.6675368144976729\n",
      "Epoch:  115, loss =  0.6673066917682184\n",
      "Epoch:  116, loss =  0.6670765690399328\n",
      "Epoch:  117, loss =  0.6668464463127666\n",
      "Epoch:  118, loss =  0.6666163235867032\n",
      "Epoch:  119, loss =  0.6663862008618251\n",
      "Epoch:  120, loss =  0.6661560781381322\n",
      "Epoch:  121, loss =  0.6659259554155587\n",
      "Epoch:  122, loss =  0.6656958326941373\n",
      "Epoch:  123, loss =  0.665465709973852\n",
      "Epoch:  124, loss =  0.6652355872547847\n",
      "Epoch:  125, loss =  0.6650054645368204\n",
      "Epoch:  126, loss =  0.6647753418199426\n",
      "Epoch:  127, loss =  0.6645452191042993\n",
      "Epoch:  128, loss =  0.6643150963897589\n",
      "Epoch:  129, loss =  0.6640849736763709\n",
      "Epoch:  130, loss =  0.6638548509641845\n",
      "Epoch:  131, loss =  0.6636247282531176\n",
      "Epoch:  132, loss =  0.6633946055432028\n",
      "Epoch:  133, loss =  0.6631644828344241\n",
      "Epoch:  134, loss =  0.6629343601267975\n",
      "Epoch:  135, loss =  0.6627042374203068\n",
      "Epoch:  136, loss =  0.6624741147149684\n",
      "Epoch:  137, loss =  0.6622439920108482\n",
      "Epoch:  138, loss =  0.6620138693077814\n",
      "Epoch:  139, loss =  0.6617837466058999\n",
      "Epoch:  140, loss =  0.6615536239052037\n",
      "Epoch:  141, loss =  0.6613235012055938\n",
      "Epoch:  142, loss =  0.6610933785071528\n",
      "Epoch:  143, loss =  0.6608632558099135\n",
      "Epoch:  144, loss =  0.6606331331138099\n",
      "Epoch:  145, loss =  0.6604030104187599\n",
      "Epoch:  146, loss =  0.6601728877249445\n",
      "Epoch:  147, loss =  0.6599427650322649\n",
      "Epoch:  148, loss =  0.6597126423407706\n",
      "Epoch:  149, loss =  0.6594825196504122\n",
      "Epoch:  150, loss =  0.659252396961173\n",
      "Epoch:  151, loss =  0.6590222742730533\n",
      "Epoch:  152, loss =  0.6587921515861023\n",
      "Epoch:  153, loss =  0.658562028900353\n",
      "Epoch:  154, loss =  0.6583319062157396\n",
      "Epoch:  155, loss =  0.658101783532262\n",
      "Epoch:  156, loss =  0.6578716608498708\n",
      "Epoch:  157, loss =  0.6576415381687307\n",
      "Epoch:  158, loss =  0.6574114154887101\n",
      "Epoch:  159, loss =  0.6571812928098582\n",
      "Epoch:  160, loss =  0.6569511701320926\n",
      "Epoch:  161, loss =  0.6567210474554959\n",
      "Epoch:  162, loss =  0.6564909247800514\n",
      "Epoch:  163, loss =  0.6562608021057923\n",
      "Epoch:  164, loss =  0.656030679432636\n",
      "Epoch:  165, loss =  0.655800556760665\n",
      "Epoch:  166, loss =  0.6555704340898791\n",
      "Epoch:  167, loss =  0.6553403114201468\n",
      "Epoch:  168, loss =  0.6551101887516492\n",
      "Epoch:  169, loss =  0.6548800660842049\n",
      "Epoch:  170, loss =  0.6546499434179954\n",
      "Epoch:  171, loss =  0.6544198207528887\n",
      "Epoch:  172, loss =  0.6541896980889345\n",
      "Epoch:  173, loss =  0.6539595754261818\n",
      "Epoch:  174, loss =  0.6537294527645485\n",
      "Epoch:  175, loss =  0.6534993301040017\n",
      "Epoch:  176, loss =  0.6532692074446895\n",
      "Epoch:  177, loss =  0.653039084786546\n",
      "Epoch:  178, loss =  0.6528089621294562\n",
      "Epoch:  179, loss =  0.6525788394735186\n",
      "Epoch:  180, loss =  0.6523487168188484\n",
      "Epoch:  181, loss =  0.6521185941652483\n",
      "Epoch:  182, loss =  0.651888471512817\n",
      "Epoch:  183, loss =  0.6516583488615051\n",
      "Epoch:  184, loss =  0.6514282262113619\n",
      "Epoch:  185, loss =  0.6511981035623545\n",
      "Epoch:  186, loss =  0.6509679809145489\n",
      "Epoch:  187, loss =  0.6507378582678297\n",
      "Epoch:  188, loss =  0.6505077356223122\n",
      "Epoch:  189, loss =  0.6502776129778481\n",
      "Epoch:  190, loss =  0.6500474903346682\n",
      "Epoch:  191, loss =  0.6498173676925582\n",
      "Epoch:  192, loss =  0.6495872450516499\n",
      "Epoch:  193, loss =  0.6493571224118609\n",
      "Epoch:  194, loss =  0.6491269997731749\n",
      "Epoch:  195, loss =  0.6488968771357235\n",
      "Epoch:  196, loss =  0.6486667544993421\n",
      "Epoch:  197, loss =  0.6484366318641295\n",
      "Epoch:  198, loss =  0.6482065092301185\n",
      "Epoch:  199, loss =  0.6479763865971939\n",
      "Epoch:  200, loss =  0.6477462639654382\n",
      "Epoch:  201, loss =  0.6475161413348841\n",
      "Epoch:  202, loss =  0.647286018705433\n",
      "Epoch:  203, loss =  0.6470558960771177\n",
      "Epoch:  204, loss =  0.6468257734499712\n",
      "Epoch:  205, loss =  0.6465956508239604\n",
      "Epoch:  206, loss =  0.646365528199135\n",
      "Epoch:  207, loss =  0.646135405575363\n",
      "Epoch:  208, loss =  0.6459052829528585\n",
      "Epoch:  209, loss =  0.6456751603314406\n",
      "Epoch:  210, loss =  0.6454450377112243\n",
      "Epoch:  211, loss =  0.6452149150921768\n",
      "Epoch:  212, loss =  0.6449847924741664\n",
      "Epoch:  213, loss =  0.6447546698573905\n",
      "Epoch:  214, loss =  0.6445245472417012\n",
      "Epoch:  215, loss =  0.644294424627197\n",
      "Epoch:  216, loss =  0.6440643020138451\n",
      "Epoch:  217, loss =  0.643834179401662\n",
      "Epoch:  218, loss =  0.6436040567905489\n",
      "Epoch:  219, loss =  0.6433739341807198\n",
      "Epoch:  220, loss =  0.6431438115719936\n",
      "Epoch:  221, loss =  0.6429136889643374\n",
      "Epoch:  222, loss =  0.64268356635785\n",
      "Epoch:  223, loss =  0.6424534437525478\n",
      "Epoch:  224, loss =  0.6422233211484143\n",
      "Epoch:  225, loss =  0.6419931985453837\n",
      "Epoch:  226, loss =  0.6417630759436043\n",
      "Epoch:  227, loss =  0.6415329533428784\n",
      "Epoch:  228, loss =  0.6413028307432553\n",
      "Epoch:  229, loss =  0.6410727081448669\n",
      "Epoch:  230, loss =  0.6408425855475979\n",
      "Epoch:  231, loss =  0.6406124629514977\n",
      "Epoch:  232, loss =  0.6403823403565826\n",
      "Epoch:  233, loss =  0.6401522177627376\n",
      "Epoch:  234, loss =  0.6399220951700448\n",
      "Epoch:  235, loss =  0.6396919725785537\n",
      "Epoch:  236, loss =  0.6394618499881985\n",
      "Epoch:  237, loss =  0.6392317273989792\n",
      "Epoch:  238, loss =  0.6390016048109285\n",
      "Epoch:  239, loss =  0.6387714822239973\n",
      "Epoch:  240, loss =  0.6385413596382512\n",
      "Epoch:  241, loss =  0.638311237053641\n",
      "Epoch:  242, loss =  0.6380811144701832\n",
      "Epoch:  243, loss =  0.6378509918878611\n",
      "Epoch:  244, loss =  0.6376208693066585\n",
      "Epoch:  245, loss =  0.637390746726674\n",
      "Epoch:  246, loss =  0.6371606241478088\n",
      "Epoch:  247, loss =  0.6369305015701289\n",
      "Epoch:  248, loss =  0.6367003789935683\n",
      "Epoch:  249, loss =  0.6364702564181601\n",
      "Epoch:  250, loss =  0.6362401338438548\n",
      "Epoch:  251, loss =  0.6360100112707348\n",
      "Epoch:  252, loss =  0.6357798886987834\n",
      "Epoch:  253, loss =  0.6355497661279185\n",
      "Epoch:  254, loss =  0.6353196435583047\n",
      "Epoch:  255, loss =  0.6350895209898267\n",
      "Epoch:  256, loss =  0.6348593984223365\n",
      "Epoch:  257, loss =  0.6346292758562125\n",
      "Epoch:  258, loss =  0.634399153291142\n",
      "Epoch:  259, loss =  0.6341690307272568\n",
      "Epoch:  260, loss =  0.6339389081644415\n",
      "Epoch:  261, loss =  0.6337087856028281\n",
      "Epoch:  262, loss =  0.6334786630423503\n",
      "Epoch:  263, loss =  0.6332485404830249\n",
      "Epoch:  264, loss =  0.6330184179248682\n",
      "Epoch:  265, loss =  0.632788295367831\n",
      "Epoch:  266, loss =  0.6325581728119132\n",
      "Epoch:  267, loss =  0.6323280502572298\n",
      "Epoch:  268, loss =  0.6320979277036824\n",
      "Epoch:  269, loss =  0.6318678051512544\n",
      "Epoch:  270, loss =  0.6316376825999621\n",
      "Epoch:  271, loss =  0.6314075600498387\n",
      "Epoch:  272, loss =  0.631177437500884\n",
      "Epoch:  273, loss =  0.6309473149530487\n",
      "Epoch:  274, loss =  0.6307171924063327\n",
      "Epoch:  275, loss =  0.6304870698608349\n",
      "Epoch:  276, loss =  0.6302569473164894\n",
      "Epoch:  277, loss =  0.6300268247732304\n",
      "Epoch:  278, loss =  0.6297967022312224\n",
      "Epoch:  279, loss =  0.6295665796902186\n",
      "Epoch:  280, loss =  0.6293364571504824\n",
      "Epoch:  281, loss =  0.6291063346118654\n",
      "Epoch:  282, loss =  0.6288762120743514\n",
      "Epoch:  283, loss =  0.6286460895380556\n",
      "Epoch:  284, loss =  0.6284159670028626\n",
      "Epoch:  285, loss =  0.628185844468822\n",
      "Epoch:  286, loss =  0.627955721935983\n",
      "Epoch:  287, loss =  0.6277255994042306\n",
      "Epoch:  288, loss =  0.6274954768736634\n",
      "Epoch:  289, loss =  0.6272653543442319\n",
      "Epoch:  290, loss =  0.6270352318159198\n",
      "Epoch:  291, loss =  0.6268051092888095\n",
      "Epoch:  292, loss =  0.6265749867628514\n",
      "Epoch:  293, loss =  0.6263448642380127\n",
      "Epoch:  294, loss =  0.6261147417143428\n",
      "Epoch:  295, loss =  0.6258846191917923\n",
      "Epoch:  296, loss =  0.6256544966703941\n",
      "Epoch:  297, loss =  0.6254243741501975\n",
      "Epoch:  298, loss =  0.6251942516311039\n",
      "Epoch:  299, loss =  0.6249641291131296\n",
      ", Final Loss =  0.6249641291131296\n",
      "  W: [[-0.00120726]\n",
      " [ 0.00181118]\n",
      " [ 0.0002318 ]\n",
      " [-0.00026236]\n",
      " [ 0.00095233]\n",
      " [-0.00098754]\n",
      " [ 0.00125718]\n",
      " [ 0.00114977]\n",
      " [ 0.0003173 ]\n",
      " [-0.00077524]], b = [0.0004549]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.00000001\n",
    "epochs = 300\n",
    "log_model = LogisticRegression(X, y)\n",
    "log_model.train(alpha, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845225d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
